from __future__ import annotations

"""
Train a small neural network (QuizNet) on synthetic arithmetic questions
generated by the existing rule-based ArithmeticGenerator.

This produces weights at quiz_model/quiznet.pt which are then used by
quiz_model.learned_model.LearnedArithmeticGenerator.
"""

from pathlib import Path
from typing import List, Tuple

import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset

from quiz_model import Difficulty, MathQuizGenerator, QuizType
from quiz_model.learned_model import (
    MAX_ABS_VAL,
    OP_TO_IDX,
    QuizNet,
    WEIGHTS_PATH,
)


def synthesize_dataset(
    n_samples: int = 10_000,
    seed: int = 123,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Generate synthetic (x, op_idx, a_idx, b_idx) using the rule-based generator.
    """
    rng = torch.Generator().manual_seed(seed)

    gen = MathQuizGenerator(seed=seed)

    diff_values: List[Difficulty] = [Difficulty.EASY, Difficulty.MEDIUM, Difficulty.HARD]
    diff_to_idx = {d: i for i, d in enumerate(diff_values)}

    xs: List[torch.Tensor] = []
    ys_op: List[int] = []
    ys_a: List[int] = []
    ys_b: List[int] = []

    for i in range(n_samples):
        # cycle difficulties to balance dataset
        difficulty = diff_values[i % len(diff_values)]
        item = gen.generate_one(quiz_type=QuizType.ARITHMETIC, difficulty=difficulty)

        operands = item.meta["operands"]
        # Some hard questions are multi-step (3 operands); for training we only
        # care about the first two operands.
        if len(operands) < 2:
            continue
        a, b = operands[0], operands[1]
        op_symbol = item.meta["operator"]

        # clamp to supported range (should already fit, but be safe)
        a = int(max(-MAX_ABS_VAL, min(MAX_ABS_VAL, a)))
        b = int(max(-MAX_ABS_VAL, min(MAX_ABS_VAL, b)))

        diff_idx = diff_to_idx[difficulty]
        diff_one_hot = torch.zeros(3)
        diff_one_hot[diff_idx] = 1.0

        noise = torch.randn(4, generator=rng)
        x = torch.cat([diff_one_hot, noise], dim=0)  # (7,)

        xs.append(x)
        ys_op.append(OP_TO_IDX[op_symbol])
        ys_a.append(a + MAX_ABS_VAL)
        ys_b.append(b + MAX_ABS_VAL)

    X = torch.stack(xs)  # (N, 7)
    y_op = torch.tensor(ys_op, dtype=torch.long)
    y_a = torch.tensor(ys_a, dtype=torch.long)
    y_b = torch.tensor(ys_b, dtype=torch.long)
    return X, y_op, y_a, y_b


def train(
    n_samples: int = 50_000,
    batch_size: int = 256,
    epochs: int = 20,
    lr: float = 1e-3,
) -> None:
    print(f"[train] synthesizing {n_samples} samples...")
    X, y_op, y_a, y_b = synthesize_dataset(n_samples=n_samples)

    dataset = TensorDataset(X, y_op, y_a, y_b)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = QuizNet()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion_op = nn.CrossEntropyLoss()
    criterion_val = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(1, epochs + 1):
        total_loss = 0.0
        for xb, yb_op, yb_a, yb_b in loader:
            logits_op, logits_a, logits_b = model(xb)
            loss_op = criterion_op(logits_op, yb_op)
            loss_a = criterion_val(logits_a, yb_a)
            loss_b = criterion_val(logits_b, yb_b)
            loss = loss_op + loss_a + loss_b

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * xb.size(0)

        avg_loss = total_loss / len(dataset)
        print(f"[epoch {epoch}] loss={avg_loss:.4f}")

    WEIGHTS_PATH.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model.state_dict(), WEIGHTS_PATH)
    print(f"[train] saved weights to {WEIGHTS_PATH}")


if __name__ == "__main__":
    train()


