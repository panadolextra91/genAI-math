from __future__ import annotations

"""
Train a small neural network (EquationNet) on synthetic equation questions
generated by the existing rule-based EquationGenerator.

This produces weights at quiz_model/quiznet_equation.pt which are then used by
quiz_model.learned_model.LearnedEquationGenerator.
"""

from typing import List, Tuple

import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset

from quiz_model import Difficulty, MathQuizGenerator, QuizType
from quiz_model.learned_model import DIFF_TO_IDX, EquationNet, WEIGHTS_PATH_EQ


def synthesize_equation_dataset(
    n_samples: int = 10_000,
    seed: int = 321,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Generate synthetic (x, a_idx, x_idx, b_idx) using the rule-based generator.
    """
    rng = torch.Generator().manual_seed(seed)

    gen = MathQuizGenerator(seed=seed)

    diff_values: List[Difficulty] = [Difficulty.EASY, Difficulty.MEDIUM, Difficulty.HARD]

    xs: List[torch.Tensor] = []
    ys_a: List[int] = []
    ys_x: List[int] = []
    ys_b: List[int] = []

    for i in range(n_samples):
        difficulty = diff_values[i % len(diff_values)]
        item = gen.generate_one(quiz_type=QuizType.EQUATION, difficulty=difficulty)

        a = int(item.meta["a"])
        b = int(item.meta["b"])
        x_val = int(item.meta["x"])

        # Map to indices consistent with EquationNet:
        # a_idx in [0, 19] for a in [1, 20]
        a = max(1, min(20, a))
        a_idx = a - 1
        # x_idx, b_idx in [0, 40] for value in [-20, 20]
        x_val = max(-20, min(20, x_val))
        b = max(-20, min(20, b))
        x_idx = x_val + 20
        b_idx = b + 20

        diff_idx = DIFF_TO_IDX[difficulty]
        diff_one_hot = torch.zeros(3)
        diff_one_hot[diff_idx] = 1.0

        noise = torch.randn(4, generator=rng)
        x = torch.cat([diff_one_hot, noise], dim=0)  # (7,)

        xs.append(x)
        ys_a.append(a_idx)
        ys_x.append(x_idx)
        ys_b.append(b_idx)

    X = torch.stack(xs)  # (N, 7)
    y_a = torch.tensor(ys_a, dtype=torch.long)
    y_x = torch.tensor(ys_x, dtype=torch.long)
    y_b = torch.tensor(ys_b, dtype=torch.long)
    return X, y_a, y_x, y_b


def train_equation(
    n_samples: int = 50_000,
    batch_size: int = 256,
    epochs: int = 20,
    lr: float = 1e-3,
) -> None:
    print(f"[train_equation] synthesizing {n_samples} samples...")
    X, y_a, y_x, y_b = synthesize_equation_dataset(n_samples=n_samples)

    dataset = TensorDataset(X, y_a, y_x, y_b)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = EquationNet()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(1, epochs + 1):
        total_loss = 0.0
        for xb, yb_a, yb_x, yb_b in loader:
            logits_a, logits_x, logits_b = model(xb)
            loss_a = criterion(logits_a, yb_a)
            loss_x = criterion(logits_x, yb_x)
            loss_b = criterion(logits_b, yb_b)
            loss = loss_a + loss_x + loss_b

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * xb.size(0)

        avg_loss = total_loss / len(dataset)
        print(f"[equation epoch {epoch}] loss={avg_loss:.4f}")

    WEIGHTS_PATH_EQ.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model.state_dict(), WEIGHTS_PATH_EQ)
    print(f"[train_equation] saved weights to {WEIGHTS_PATH_EQ}")


if __name__ == "__main__":
    train_equation()


